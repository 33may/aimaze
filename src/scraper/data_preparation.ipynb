{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ec745bf55d9a3a83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Data Preparation Overview\n",
    "\n",
    "This document describes the preparation of data used to support the development and evaluation of algorithms for retrieving and labeling API documentation pages. The process includes collecting raw HTML documents, labeling relevant pages as endpoint-related, and storing all data in a structured format for later use.\n",
    "\n",
    "\n",
    "## 1. Data Collection Objectives\n",
    "\n",
    "the data in our case consists of raw HTML pages from multiple API documentation websites. Based on the HTML documentâ€”referred to here as an *API page* we need to perform two tasks. To better understand the tasks, we introduce one more distinction: the set of *API pages* contains the *endpoint pages* (API pages that actually contain the endpoints that need to be implemented) and their complement *non-endpoint pages*, which we are not interested in. Note here that there are also non-endpoint pages that contain general information about the API, which might be required to properly generate the endpoints, but that will be covered later during development.\n",
    "\n",
    "\n",
    "## 2. Tasks\n",
    "\n",
    "Two main tasks in the project are defined as:\n",
    "\n",
    "#### Task 1: Given the website link, parse the website and retrieve all API pages\n",
    "\n",
    "#### Task 2: Using the API pages generate the python code to interact with endpoints.\n",
    "\n",
    "The data preparation aims to make both these task easier during development by providing structured data infrastructure that will ensure more modular development and save time allowing to interact with application parts without need to rely on other parts.\n",
    "\n",
    "## 3. Benchmark Construction\n",
    "\n",
    "The website parsing requires not only saving the pages but also the good algorithm that will ensure all the endpoints are collected without too many additional irrelevant pages, for that we need to design the algorithm.\n",
    "\n",
    "For the algorith design we decided to make the benchmark driven approach, where we construct the benchmark that after the run of parsing process with certain algorithm gives the result statistics. To make this benchmark we review API docs and labeled the Endpoint docs.\n",
    "\n",
    "Example object:\n",
    "```\n",
    "[\n",
    "      {\n",
    "          \"name\": \"aNewSpring\",\n",
    "          \"base_url\": \"https://support.anewspring.com\",\n",
    "          \"domain_url\": \"/en/\",\n",
    "          \"endpoint_pages\" :\n",
    "                [\n",
    "                    \"https://support.anewspring.com/en/articles/70415-api-calls-users\",\n",
    "                    \"https://support.anewspring.com/en/articles/70416-api-calls-user-groups\",\n",
    "                    \"https://support.anewspring.com/en/articles/70417-api-calls-subenvironments\",\n",
    "                    \"https://support.anewspring.com/en/articles/70418-api-calls-subscriptions\",\n",
    "                    \"https://support.anewspring.com/en/articles/70407-api-calls-templates-and-courses\",\n",
    "                    \"https://support.anewspring.com/en/articles/70626-api-calls-bundles\",\n",
    "                    \"https://support.anewspring.com/en/articles/70419-api-calls-calendar-items\",\n",
    "                    \"https://support.anewspring.com/en/articles/70420-api-calls-access-codes\",\n",
    "                    \"https://support.anewspring.com/en/articles/70421-api-calls-events\",\n",
    "                    \"https://support.anewspring.com/en/articles/70588-api-calls-profile\"\n",
    "                ]\n",
    "      }\n",
    "]\n",
    "```\n",
    "\n",
    "However the webparsing process is time-consuming and this is suboptimal to wait each time to evaluate algorithm, saving pages are also required for easier modular development of code generation. We created the database table for development\n"
   ],
   "id": "d8f1bad830db0bd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 4. Database Schema\n",
    "\n",
    "To support modular and repeatable development, all scraped pages and their labels are stored in a local database. The table structure is as follows:\n",
    "\n",
    "| Column | Type | Description|\n",
    "|------|-----|----------------------------------------------|\n",
    "| `url`| TEXT| Primary key. Full URL of the scraped page|\n",
    "| `html`| TEXT| Raw HTML content of the page|\n",
    "| `is_endpoint`| BOOLEAN| Whether the page is an endpoint page|\n"
   ],
   "id": "bd7e9136b8964f5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These functions are defined in the `db_utils`, here I have duplicated them to show you.",
   "id": "54c5e868acc97d1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "from src.scraper.scraper import bfs_site\n",
    "from src.scraper.eval_db.db_utils import get_connection\n",
    "\n",
    "\n",
    "def init_db():\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('''\n",
    "        create table if not exists pages (\n",
    "            url TEXT primary key,\n",
    "            html TEXT,\n",
    "            is_endpoint BOOLEAN)\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def get_page_by_url(url:str) -> str:\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT html FROM pages WHERE url = %s\", (url,))\n",
    "    row = cur.fetchone()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return row[0] if row else None\n",
    "\n",
    "def get_api_pages_by_url(url: str) -> list:\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT html FROM pages WHERE url LIKE %s and is_endpoint = True\", (f\"%{url}%\",))\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "def is_site_in_db(url: str) -> bool:\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT 1 FROM pages WHERE url LIKE %s LIMIT 1\", (f\"%{url}%\",))\n",
    "    exists = cur.fetchone() is not None\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return exists\n",
    "\n",
    "def store_page(url: str, html: str, is_endpoint: bool) -> None:\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('''\n",
    "        INSERT INTO pages (url, html, is_endpoint)\n",
    "        VALUES (%s, %s, %s)\n",
    "        on conflict do nothing\n",
    "    ''', (url, html, is_endpoint))\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def add_api_to_db(url: str, domain_url: str) -> None:\n",
    "\n",
    "    with open('../benchmark_data.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for obj in data:\n",
    "            if obj.get(\"base_url\") == url:\n",
    "                endpoint_pages = obj.get(\"endpoint_pages\")\n",
    "\n",
    "\n",
    "    data = bfs_site(url, lambda content: True, domain_url)\n",
    "\n",
    "    all_scraped_pages = data.get(\"endpoint_pages\", {})\n",
    "\n",
    "    for key, value in all_scraped_pages.items():\n",
    "        is_endpoint = key in endpoint_pages\n",
    "        store_page(key, value, is_endpoint)"
   ],
   "id": "f81636af7ce1aff7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "in the `add_api_to_db` function we have the `bfs_site` function, since we want to parse all the pages from the website, we pass:\n",
    "\n",
    "```python\n",
    "lambda content: True\n",
    "```\n",
    "\n",
    "That filters and adds all the pages to the result.\n",
    "\n",
    "All collected data, including both labeled and unlabeled pages, is saved. This allows re-use of the same data for different parsing strategies or model experiments without re-scraping.\n",
    "\n",
    "This database serve as mimic of the internet, with small adjustment to the `bfs_site` function we could retrieve the html from database by link using `get_page_by_url` and then use `beutifulsoup` library to work with the html just like it was received from the web request."
   ],
   "id": "cd888d82cbe5383f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 5. Workflow and Labeling Logic\n",
    "\n",
    "1. The benchmark JSON file is loaded.\n",
    "2. The crawler executes a breadth-first crawl starting from the `base_url`, staying within the domain.\n",
    "3. All reachable pages are saved with their HTML content.\n",
    "4. Each page is checked against the benchmark `endpoint_pages`. If it is found there, it is labeled as `is_endpoint = True`.\n",
    "5. The complete set of pages and labels is stored in the database.\n",
    "\n",
    "This process separates data collection from downstream tasks and enables reproducible evaluation of new crawling strategies and model variants.\n",
    "\n",
    "\n",
    "## 6. Assumptions for Downstream Development\n",
    "\n",
    "- Crawling and labeling are performed once and stored persistently.\n",
    "- All downstream code assumes access to the database rather than repeating the crawling step.\n",
    "- The endpoint pages are used for tasks like code generation or API schema inference.\n",
    "\n",
    "This allows for fast iteration and independent development of later components.\n"
   ],
   "id": "42c0a979f5af7348"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
