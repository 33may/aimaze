{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Data Preparation Overview\n",
    "\n",
    "This document describes the preparation of data used to support the development and evaluation of algorithms for retrieving and labeling API documentation pages. The process includes collecting raw HTML documents, labeling relevant pages as endpoint-related, and storing all data in a structured format for later use.\n",
    "\n",
    "\n",
    "## Data Collection Objectives\n",
    "\n",
    "the data in our case consists of raw HTML pages from multiple API documentation websites. Based on the HTML document—referred to here as an **API page** we need to perform two tasks. To better understand the tasks, we introduce one more distinction: the set of **API pages** contains the **endpoint pages** (API pages that actually contain the endpoints that need to be implemented) and their complement **non-endpoint pages**, which we are not interested in. Note here that there are also non-endpoint pages that contain general information about the API, which might be required to properly generate the endpoints, but that will be covered later during development.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "Two main tasks in the project are defined as:\n",
    "\n",
    "#### Task 1: Given the website link, parse the website and retrieve all API pages\n",
    "\n",
    "#### Task 2: Using the API pages generate the python code to interact with endpoints.\n",
    "\n",
    "The data preparation aims to make both these task easier during development by providing structured data infrastructure that will ensure more modular development and save time allowing to interact with application parts without need to rely on other parts.\n",
    "\n",
    "## Benchmark Construction\n",
    "\n",
    "The website parsing requires not only saving the pages but also the good algorithm that will ensure all the endpoints are collected without too many additional irrelevant pages, for that we need to design the algorithm.\n",
    "\n",
    "For the algorith design we decided to make the benchmark driven approach, where we construct the benchmark that after the run of parsing process with certain algorithm gives the result statistics. To make this benchmark we review API docs and labeled the Endpoint docs.\n",
    "\n",
    "From the list of APIs we constructed the labeled dataset, for each site we have created the binary classification labeling of the **Endpoint pages**.\n",
    "\n",
    "Example object:\n",
    "```json\n",
    "[\n",
    "      {\n",
    "          \"name\": \"aNewSpring\",\n",
    "          \"base_url\": \"https://support.anewspring.com\",\n",
    "          \"domain_url\": \"/en/\",\n",
    "          \"endpoint_pages\" :\n",
    "                [\n",
    "                    \"https://support.anewspring.com/en/articles/70415-api-calls-users\",\n",
    "                    \"https://support.anewspring.com/en/articles/70416-api-calls-user-groups\",\n",
    "                    \"https://support.anewspring.com/en/articles/70417-api-calls-subenvironments\",\n",
    "                    \"https://support.anewspring.com/en/articles/70418-api-calls-subscriptions\",\n",
    "                    \"https://support.anewspring.com/en/articles/70407-api-calls-templates-and-courses\",\n",
    "                    \"https://support.anewspring.com/en/articles/70626-api-calls-bundles\",\n",
    "                    \"https://support.anewspring.com/en/articles/70419-api-calls-calendar-items\",\n",
    "                    \"https://support.anewspring.com/en/articles/70420-api-calls-access-codes\",\n",
    "                    \"https://support.anewspring.com/en/articles/70421-api-calls-events\",\n",
    "                    \"https://support.anewspring.com/en/articles/70588-api-calls-profile\"\n",
    "                ]\n",
    "      }\n",
    "]\n",
    "```\n",
    "\n",
    "However the webparsing process is time-consuming and this is suboptimal to wait each time to evaluate algorithm, saving pages are also required for easier modular development of code generation. We created the database table for development. So such approach allows for both running the `scraper_benchmark` function to evaluate scraping algorithms way faster(comparison in the next section), and retrieving the all HTML **endpoint pages** to test the coder part of the project, without need to run the scraping, which might not be optimal and takes a lot of time. This will allow to develop the coder part from the start of the project with the assumption that optimal scraping logic is already developed.\n"
   ],
   "id": "d8f1bad830db0bd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Database Schema\n",
    "\n",
    "To support modular and repeatable development, all scraped pages and their labels are stored in a local database. The table structure is as follows:\n",
    "\n",
    "| Column | Type | Description|\n",
    "|------|-----|----------------------------------------------|\n",
    "| `url`| TEXT| Primary key. Full URL of the scraped page|\n",
    "| `html`| TEXT| Raw HTML content of the page|\n",
    "| `is_endpoint`| BOOLEAN| Whether the page is an endpoint page|\n"
   ],
   "id": "bd7e9136b8964f5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These functions are defined in the `db_utils`, here I have duplicated them to show in this notebook.",
   "id": "54c5e868acc97d1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "from src.scraper.scraper import bfs_site\n",
    "from src.scraper.eval_db.db_utils import get_connection\n",
    "\n",
    "\n",
    "def init_db():\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('''\n",
    "        create table if not exists pages (\n",
    "            url TEXT primary key,\n",
    "            html TEXT,\n",
    "            is_endpoint BOOLEAN)\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def get_page_by_url(url:str) -> str:\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT html FROM pages WHERE url = %s\", (url,))\n",
    "    row = cur.fetchone()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return row[0] if row else None\n",
    "\n",
    "def get_api_pages_by_url(url: str) -> list:\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT html FROM pages WHERE url LIKE %s and is_endpoint = True\", (f\"%{url}%\",))\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "def is_site_in_db(url: str) -> bool:\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT 1 FROM pages WHERE url LIKE %s LIMIT 1\", (f\"%{url}%\",))\n",
    "    exists = cur.fetchone() is not None\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return exists\n",
    "\n",
    "def store_page(url: str, html: str, is_endpoint: bool) -> None:\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('''\n",
    "        INSERT INTO pages (url, html, is_endpoint)\n",
    "        VALUES (%s, %s, %s)\n",
    "        on conflict do nothing\n",
    "    ''', (url, html, is_endpoint))\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def add_api_to_db(url: str, domain_url: str) -> None:\n",
    "\n",
    "    with open('../benchmark_data.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for obj in data:\n",
    "            if obj.get(\"base_url\") == url:\n",
    "                endpoint_pages = obj.get(\"endpoint_pages\")\n",
    "\n",
    "\n",
    "    data = bfs_site(url, lambda content: True, domain_url)\n",
    "\n",
    "    all_scraped_pages = data.get(\"endpoint_pages\", {})\n",
    "\n",
    "    for key, value in all_scraped_pages.items():\n",
    "        is_endpoint = key in endpoint_pages\n",
    "        store_page(key, value, is_endpoint)"
   ],
   "id": "f81636af7ce1aff7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Workflow and Labeling Logic\n",
    "\n",
    "1. The benchmark JSON file is loaded.\n",
    "2. The crawler executes a breadth-first crawl starting from the `base_url`, staying within the domain.\n",
    "3. All reachable pages are saved with their HTML content.\n",
    "4. Each page is checked against the benchmark `endpoint_pages`. If it is found there, it is labeled as `is_endpoint = True`.\n",
    "5. The complete set of pages and labels is stored in the database.\n",
    "\n",
    "This process separates data collection from downstream tasks and enables reproducible evaluation of new crawling strategies and model variants.\n",
    "\n",
    "in the `add_api_to_db` function we have the `bfs_site` function, since we want to parse all the pages from the website and save them to db, we pass:\n",
    "\n",
    "```python\n",
    "lambda content: True\n",
    "```\n",
    "\n",
    "That filters and adds all the pages to the result.\n",
    "\n",
    "All collected data, including both labeled and unlabeled pages, is saved. This allows re-use of the same data for different parsing strategies or model experiments without re-scraping.\n",
    "\n",
    "This database serve as mimic of the internet, with small adjustment to the `bfs_site` function we now could retrieve the html from database by link using `get_page_by_url` and then use `beutifulsoup` library to work with the html just like it was received from the web request."
   ],
   "id": "cd888d82cbe5383f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Time Comparison\n",
    "\n",
    "Here we evaluate both versions to show the time difference between running the web parsing and db mimic."
   ],
   "id": "43bd527c750072be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T11:50:22.830299Z",
     "start_time": "2025-04-18T11:28:40.504008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from src.scraper.scraper_benchmark import benchmark_scraper\n",
    "\n",
    "# Time live scraping\n",
    "start_live = time.perf_counter()\n",
    "benchmark_scraper(lambda content: True, num_apis=1, use_db=False, log=False)\n",
    "end_live = time.perf_counter()\n",
    "live_duration = end_live - start_live\n",
    "print(f\"Live scraping took {live_duration:.2f} seconds\")\n",
    "\n",
    "# Time database version\n",
    "start_db = time.perf_counter()\n",
    "benchmark_scraper(lambda content: True, num_apis=1, use_db=True, log=False)\n",
    "end_db = time.perf_counter()\n",
    "db_duration = end_db - start_db\n",
    "print(f\"Database access took {db_duration:.2f} seconds\")"
   ],
   "id": "e94fb6260c95a076",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-04-18 13:28:40.511\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.scraper.scraper_benchmark\u001B[0m:\u001B[36mbenchmark_scraper\u001B[0m:\u001B[36m75\u001B[0m - \u001B[1mBenchmarking API: aNewSpring (https://support.anewspring.com)\u001B[0m\n",
      "\u001B[32m2025-04-18 13:49:50.613\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.scraper.scraper_benchmark\u001B[0m:\u001B[36mbenchmark_scraper\u001B[0m:\u001B[36m98\u001B[0m - \u001B[1mAPI aNewSpring: Expected 10, Scraped 596, Matched 10, Coverage 100.00%\u001B[0m\n",
      "\u001B[32m2025-04-18 13:49:50.620\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.scraper.scraper_benchmark\u001B[0m:\u001B[36mbenchmark_scraper\u001B[0m:\u001B[36m119\u001B[0m - \u001B[1mBenchmark completed. Summary written to benchmark_summary.json\u001B[0m\n",
      "\u001B[32m2025-04-18 13:49:50.660\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.scraper.scraper_benchmark\u001B[0m:\u001B[36mbenchmark_scraper\u001B[0m:\u001B[36m75\u001B[0m - \u001B[1mBenchmarking API: aNewSpring (https://support.anewspring.com)\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live scraping took 1270.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-04-18 13:50:22.819\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.scraper.scraper_benchmark\u001B[0m:\u001B[36mbenchmark_scraper\u001B[0m:\u001B[36m98\u001B[0m - \u001B[1mAPI aNewSpring: Expected 10, Scraped 596, Matched 10, Coverage 100.00%\u001B[0m\n",
      "\u001B[32m2025-04-18 13:50:22.823\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.scraper.scraper_benchmark\u001B[0m:\u001B[36mbenchmark_scraper\u001B[0m:\u001B[36m119\u001B[0m - \u001B[1mBenchmark completed. Summary written to benchmark_summary.json\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database access took 32.17 seconds\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T11:50:22.920301Z",
     "start_time": "2025-04-18T11:50:22.914725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speedup_factor = live_duration / db_duration if db_duration else float('inf')\n",
    "\n",
    "print(\"\\n--- Benchmark Comparison ---\")\n",
    "print(f\"Live scraping: {live_duration:.2f} seconds\")\n",
    "print(f\"Database use : {db_duration:.2f} seconds\")\n",
    "print(f\"Speedup      : {speedup_factor:.2f}× faster\")"
   ],
   "id": "67ac8b87b9e01741",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmark Comparison ---\n",
      "Live scraping: 1270.20 seconds\n",
      "Database use : 32.17 seconds\n",
      "Speedup      : 39.48× faster\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As observed from the comparison, the difference in time is dramatic, suggesting that using db is crucial to ensure stable and efficient development.",
   "id": "b7f9720055c53645"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The result of the benchmark is saved and stored in separate file, for this run:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"overall\": {\n",
    "        \"total_expected\": 10,\n",
    "        \"total_scraped\": 596,\n",
    "        \"total_matched\": 10,\n",
    "        \"overall_coverage\": 1.0\n",
    "    },\n",
    "    \"apis\": {\n",
    "        \"aNewSpring\": {\n",
    "            \"expected\": 10,\n",
    "            \"scraped\": 596,\n",
    "            \"matched\": 10,\n",
    "            \"coverage\": 1.0,\n",
    "            \"missing\": [],\n",
    "            \"extra\": [\n",
    "                \"https://support.anewspring.com/en/articles/45857-play-content-from-another-system-in-anewspring-using-lti-1-1-consumer\",\n",
    "                \"https://support.anewspring.com/en/articles/70600-questions-fill-in-the-blanks-text-content-converter\",\n",
    "                \"...\",\n",
    "                \"https://support.anewspring.com/en/articles/32177-set-up-a-hand-in-assignment\",\n",
    "                \"https://support.anewspring.com/en/articles/70435\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ],
   "id": "6cb56d3cdc36795d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Coder\n",
    "With the data preparation and storage infrastructure in place, the development of the coder component becomes fully decoupled from the web scraping and parsing logic. This separation of concerns introduces several advantages:\n",
    "\n",
    "The coder can now operate directly on the stored HTML of endpoint pages, retrieved from the local database. This eliminates the need for live scraping during development and testing, making iteration significantly faster and more reliable.\n",
    "\n",
    "All input data for the coder is available in a consistent and structured format. Each endpoint page is accessible via its URL, with the full HTML content stored locally. This allows deterministic behavior and easier debugging.\n",
    "\n",
    "The coder module focuses purely on transforming HTML documents into structured Python code for interacting with APIs. It does not handle URL traversal, filtering, or network requests. This strict encapsulation ensures a cleaner and more maintainable system architecture.\n",
    "\n",
    "This design ensures that the coder component is modular, testable, and production-ready. It enables isolated improvement of code generation strategies while preserving compatibility with the broader data pipeline."
   ],
   "id": "5432b526a17c249c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "This data preparation process provided a robust foundation for modular development, benchmarking, and experimentation across different components of the application. By separating concerns between data acquisition, labeling, and downstream usage, we achieved several key benefits:\n",
    "\n",
    "- **Reusability and Modularity**: Once pages are scraped and stored, they can be reused across multiple phases—benchmarking, endpoint classification, and code generation—without re-crawling the source websites. This allows developers to work independently on different parts of the system.\n",
    "\n",
    "- **Benchmarking Support**: The labeled dataset of known endpoint pages enables objective evaluation of different crawling strategies. By comparing parsed output against the benchmark, we can iteratively improve scraping algorithms with measurable feedback.\n",
    "\n",
    "- **Efficient Experimentation**: The database acts as a local mirror of the documentation sites. This allows for rapid testing of filters, heuristics, and extraction techniques without repeated network requests, significantly speeding up development.\n",
    "\n",
    "- **Seamless Integration for Code Generation**: With endpoint pages readily available in the database, the code generation module can be developed and tested independently of the scraper. This decouples the system and ensures that early work on code generation is not blocked by scraping performance.\n",
    "\n",
    "- **Controlled Environment for Iteration**: The data pipeline enables deterministic runs with consistent input, which is crucial for debugging, regression testing, and ensuring reproducibility in research and production environments.\n",
    "\n",
    "In short, this data preparation step transforms an inherently messy and variable process—web crawling and API documentation analysis—into a structured, testable, and iterative workflow. It enables the project to be developed as a pipeline of loosely coupled modules, each of which can be tested, benchmarked, and improved in isolation.\n"
   ],
   "id": "42c0a979f5af7348"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
